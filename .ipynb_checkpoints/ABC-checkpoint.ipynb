{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Machine Translation](#1.-Machine-Translation)\n",
    "2. [Neural Machine Translation](#2.-Neural-Machine-Translation)\n",
    "3. [Loading the dataset](#3.-Loading-the-dataset)\n",
    "4. [Preparing the Text Data](#4.-Preparing-the-Text-Data)\n",
    "5. [Build, Train Model](#5.-Build,-Train-Model)\n",
    "6. [Evaluate Model](#6.-Evaluate-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we hear about language translation, the first thing that comes to mind is Google Translate!!!, it's such a life savior. It has changed the world by allowing people to communicate when it wouldn’t otherwise be possible.<br>\n",
    "Machine translation, sometimes referred to by the abbreviation MT is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.(Source - Wikipedia)<br><br>\n",
    "***\n",
    "As famously quoted by the late Susan Sontag,<br>\n",
    "> _\"Translation is the circulatory system of the world's literatures\"_\n",
    "***\n",
    "\n",
    "\n",
    "Machine translation is the task of automatically converting source text in one language to text in another language. Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language. It is a challenging task that traditionally involves large statistical models developed using highly sophisticated linguistic knowledge.<br>\n",
    "**Classical machine translation** methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT. The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural machine translation, or NMT for short, is the use of neural network models to learn a statistical model for machine translation. The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning.<br>\n",
    "As such, neural machine translation systems are said to be _end-to-end systems_ as only one model is required for the translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Perceptron neural network models can be used for machine translation, although the models are limited by a fixed-length input sequence where the output must be the same length.<br>\n",
    "These early models have been greatly improved upon recently through the use of recurrent neural networks(RNNs) organized into an encoder-decoder architecture that allow for variable length input and output sequences.<br>\n",
    "As stated in [Neural Machine Translation by Jointly Learning to Align and Translate, 2014](https://arxiv.org/abs/1409.0473),\n",
    ">An encoder neural network reads and encodes a source sentence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.\n",
    "\n",
    "Key to the encoder-decoder architecture is the ability of the model to encode the source text into an internal fixed-length representation called the context vector. Interestingly, once encoded, different decoding systems could be used, in principle, to translate the context into different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoders with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence.<br>\n",
    "The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded.<br>\n",
    "> Using a fixed-sized representation to capture all the semantic details of a very long sentence is very difficult. A more efficient approach, however, is to read the whole sentence or paragraph, then to produce the translated words one at a time, each time focusing on a different part of the input sentence to gather the semantic details required to produce the next output word.\n",
    "\n",
    "The encoder-decoder recurrent neural network architecture with attention is currently the state-of-the-art on some benchmark problems for machine translation. And this architecture is used in the heart of the Google Neural Machine Translation system, or GNMT, used in their Google Translate service.Although effective, the neural machine translation systems still suffer some issues, such as scaling to larger vocabularies of words and the slow speed of training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will develop a neural machine translation system for translating German phrases to English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on [ManyThings.org](http://www.manythings.org/anki/) which comprises of German phrases and their English counterparts.<br><br>\n",
    "Dataset: German–English deu-eng.zip<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzipping the file and putting it in destination folder\n",
    "!unzip 'data/deu-eng.zip' -d 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping the file returns a file _deu.txt_ that has pairs of English to German phrases, one per line with a tab separating the language. We will frame the prediction problem as given a sequence of words in German as input, translate or predict the sequence of words in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to prepare the text data ready for modeling. It's very important to know the steps we will be performing as far as data cleaning process is concerned since it can vary depending on the datasets.<br>\n",
    "Few observations from the dataset we have at our disposal,\n",
    "- Symbols like punctuation and other special characters\n",
    "- The text contains uppercase and lowercase.\n",
    "- There are duplicate phrases in English with different translations in German.\n",
    "- The file is ordered by sentence length with very long sentences toward the end of the file.\n",
    "\n",
    "A good text cleaning procedure may handle some or all of these observations. Data preparation is divided into two subsections:\n",
    "- Clean Text\n",
    "- Split Text into Train, Test dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data in a way that preserves the Unicode German characters. The function below called `load_doc()` will load the file as a blob of text.<br>\n",
    "Each line contains a single pair of phrases, first English and then German, separated by a *tab character*. We must split the loaded text by line and then by phrase. The function `to_pairs()` will split the loaded text.<br><br>\n",
    "We are now ready to clean sentence. The specific cleaning operations we will perform are as follows:\n",
    "- Remove all non-printable characters.\n",
    "- Remove all punctuation characters.\n",
    "- Normalize all Unicode characters to ASCII (e.g. Latin characters).\n",
    "- Normalize the case to lowercase.\n",
    "- Remove any remaining tokens that are not alphabetic.\n",
    "\n",
    "We will perform these operations on each phrase for each pair in the loaded dataset. The `clean_pairs()` function below implements these operations.<br>\n",
    "\n",
    "Finally, now that the data has been cleaned, we can save the list of phrase pairs to a file ready for use. The function `save_clean_data()` uses the pickle API to save the list of clean text to file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load doc into memory\n",
    "def load_doc(filename):\n",
    "    #open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#Split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    #prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    #prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)\n",
    "                       \n",
    "#Save a list of clean sentences to a file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to load the dataset and apply the above defined-function(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/english-german.pkl\n",
      "[hi]=> [hallo]\n",
      "[hi]=> [gru gott]\n",
      "[run]=> [lauf]\n",
      "[wow]=> [potzdonner]\n",
      "[wow]=> [donnerwetter]\n",
      "[fire]=> [feuer]\n",
      "[help]=> [hilfe]\n",
      "[help]=> [zu hulf]\n",
      "[stop]=> [stopp]\n",
      "[wait]=> [warte]\n",
      "[go on]=> [mach weiter]\n",
      "[hello]=> [hallo]\n",
      "[i ran]=> [ich rannte]\n",
      "[i see]=> [ich verstehe]\n",
      "[i see]=> [aha]\n",
      "[i try]=> [ich probiere es]\n",
      "[i won]=> [ich hab gewonnen]\n",
      "[i won]=> [ich habe gewonnen]\n",
      "[smile]=> [lacheln]\n",
      "[cheers]=> [zum wohl]\n",
      "[freeze]=> [keine bewegung]\n",
      "[freeze]=> [stehenbleiben]\n",
      "[got it]=> [kapiert]\n",
      "[got it]=> [verstanden]\n",
      "[got it]=> [einverstanden]\n",
      "[he ran]=> [er rannte]\n",
      "[he ran]=> [er lief]\n",
      "[hop in]=> [mach mit]\n",
      "[hug me]=> [druck mich]\n",
      "[hug me]=> [nimm mich in den arm]\n",
      "[hug me]=> [umarme mich]\n",
      "[i fell]=> [ich fiel]\n",
      "[i fell]=> [ich fiel hin]\n",
      "[i fell]=> [ich sturzte]\n",
      "[i fell]=> [ich bin hingefallen]\n",
      "[i fell]=> [ich bin gesturzt]\n",
      "[i know]=> [ich wei]\n",
      "[i lied]=> [ich habe gelogen]\n",
      "[i lost]=> [ich habe verloren]\n",
      "[i paid]=> [ich habe bezahlt]\n",
      "[i paid]=> [ich zahlte]\n",
      "[i sang]=> [ich sang]\n",
      "[i swim]=> [ich schwimme]\n",
      "[im]=> [ich bin jahre alt]\n",
      "[im]=> [ich bin]\n",
      "[im ok]=> [mir gehts gut]\n",
      "[im ok]=> [es geht mir gut]\n",
      "[im up]=> [ich bin wach]\n",
      "[im up]=> [ich bin auf]\n",
      "[no way]=> [unmoglich]\n",
      "[no way]=> [das kommt nicht in frage]\n",
      "[no way]=> [das gibts doch nicht]\n",
      "[no way]=> [ausgeschlossen]\n",
      "[no way]=> [in keinster weise]\n",
      "[really]=> [wirklich]\n",
      "[really]=> [echt]\n",
      "[really]=> [im ernst]\n",
      "[thanks]=> [danke]\n",
      "[try it]=> [versuchs]\n",
      "[we won]=> [wir haben gewonnen]\n",
      "[why me]=> [warum ich]\n",
      "[ask tom]=> [frag tom]\n",
      "[ask tom]=> [fragen sie tom]\n",
      "[ask tom]=> [fragt tom]\n",
      "[awesome]=> [fantastisch]\n",
      "[be cool]=> [entspann dich]\n",
      "[be fair]=> [sei nicht ungerecht]\n",
      "[be fair]=> [sei fair]\n",
      "[be nice]=> [sei nett]\n",
      "[be nice]=> [seien sie nett]\n",
      "[beat it]=> [geh weg]\n",
      "[beat it]=> [hau ab]\n",
      "[beat it]=> [verschwinde]\n",
      "[beat it]=> [verdufte]\n",
      "[beat it]=> [mach dich fort]\n",
      "[beat it]=> [zieh leine]\n",
      "[beat it]=> [mach dich vom acker]\n",
      "[beat it]=> [verzieh dich]\n",
      "[beat it]=> [verkrumele dich]\n",
      "[beat it]=> [troll dich]\n",
      "[beat it]=> [zisch ab]\n",
      "[beat it]=> [pack dich]\n",
      "[beat it]=> [mach ne fliege]\n",
      "[beat it]=> [schwirr ab]\n",
      "[beat it]=> [mach die sause]\n",
      "[beat it]=> [scher dich weg]\n",
      "[beat it]=> [scher dich fort]\n",
      "[call me]=> [ruf mich an]\n",
      "[come in]=> [komm herein]\n",
      "[come in]=> [herein]\n",
      "[come on]=> [komm]\n",
      "[come on]=> [kommt]\n",
      "[come on]=> [mach schon]\n",
      "[come on]=> [macht schon]\n",
      "[come on]=> [komm schon]\n",
      "[get tom]=> [hol tom]\n",
      "[get out]=> [raus]\n",
      "[get out]=> [geht raus]\n",
      "[get out]=> [geh raus]\n",
      "[get out]=> [geht raus]\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "filename = 'data/deu.txt'\n",
    "doc = load_doc(filename)\n",
    "#Split into English-German pairs\n",
    "pairs = to_pairs(doc)\n",
    "#Clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "#Save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'data/english-german.pkl')\n",
    "#Verifying if the functions work as intended\n",
    "for i in range(100):\n",
    "    print('[%s]=> [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Text into Train, Test dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean data contains a little over 195,000 phrase pairs and some of the pairs towards the end of the file are very long.<br>\n",
    "This is a good number of examples for developing a small translation model. The complexity of the model increases with the number of examples, length of phrases, and size of the vocabulary.<br>\n",
    "Although we have a good dataset for modeling translation, we will simplify the problem slightly to dramatically reduce the size of the model required, and in turn the training time required to fit the model.<br>\n",
    "We will simplify the problem by reducing the dataset to the first 10,000 examples in the file; these will be the shortest phrases in the dataset. Further, we will consider the first 9,000 of those as examples for training and the remaining 1,000 examples to test the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdsfghj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build, Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
